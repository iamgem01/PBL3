import { GoogleGenerativeAI } from '@google/generative-ai';
import dotenv from 'dotenv';

dotenv.config();


interface ModelConfig {
    apiKey: string;
    modelName: string;
}


function getApiKeys(): string[] {
    const keys: string[] = [];
    let index = 1;

    if (process.env.GEMINI_API_KEY) {
        keys.push(process.env.GEMINI_API_KEY);
    }

    while (process.env[`GEMINI_API_KEY_${index}`]) {
        keys.push(process.env[`GEMINI_API_KEY_${index}`]!);
        index++;
    }

    if (keys.length === 0) {
        throw new Error('GEMINI_API_KEY is not set in environment variables');
    }

    return keys;
}

function getModels(): string[] {
    const models: string[] = [];

    const defaultModel = process.env.GEMINI_MODEL || 'gemini-2.0-flash-exp';
    models.push(defaultModel);

    let index = 1;
    while (process.env[`GEMINI_MODEL_${index}`]) {
        models.push(process.env[`GEMINI_MODEL_${index}`]!);
        index++;
    }

    return models;
}

function createModelConfigs(): ModelConfig[] {
    const apiKeys = getApiKeys();
    const models = getModels();
    const configs: ModelConfig[] = [];

    for (const apiKey of apiKeys) {
        for (const model of models) {
            configs.push({ apiKey, modelName: model });
        }
    }

    return configs;
}

function isQuotaOrRateLimitError(error: any): boolean {
    const errorMessage = error?.message?.toLowerCase() || '';
    const statusCode = error?.status || error?.code;

    return (
        errorMessage.includes('quota') ||
        errorMessage.includes('rate limit') ||
        errorMessage.includes('429') ||
        errorMessage.includes('resource exhausted') ||
        statusCode === 429 ||
        statusCode === 403 ||
        errorMessage.includes('permission denied')
    );
}

export class GeminiService {
    private modelConfigs: ModelConfig[];
    private currentConfigIndex: number = 0;
    private failedConfigs: Set<number> = new Set(); // Track failed configs
    private lastResetTime: number = Date.now();
    private readonly RESET_INTERVAL = 5 * 60 * 1000; // Reset sau 5 ph√∫t

    constructor() {
        this.modelConfigs = createModelConfigs();
        if (this.modelConfigs.length === 0) {
            throw new Error('No Gemini API keys or models configured');
        }
        console.log(`Initialized with ${this.modelConfigs.length} model configurations`);
    }

    private getCurrentModel() {
        const config = this.modelConfigs[this.currentConfigIndex];
        const genAI = new GoogleGenerativeAI(config.apiKey);
        return genAI.getGenerativeModel({ model: config.modelName });
    }

    // Reset failed configs sau m·ªôt kho·∫£ng th·ªùi gian
    private resetFailedConfigsIfNeeded() {
        const now = Date.now();
        if (now - this.lastResetTime > this.RESET_INTERVAL) {
            this.failedConfigs.clear();
            this.lastResetTime = now;
            console.log('üîÑ Resetting failed configs - retrying all keys');
        }
    }

    // T√¨m config ti·∫øp theo ch∆∞a b·ªã failed
    private getNextAvailableConfigIndex(): number | null {
        this.resetFailedConfigsIfNeeded();

        const startIndex = this.currentConfigIndex;
        let attempts = 0;

        do {
            this.currentConfigIndex = (this.currentConfigIndex + 1) % this.modelConfigs.length;
            attempts++;

            // N·∫øu ƒë√£ th·ª≠ h·∫øt t·∫•t c·∫£ configs
            if (attempts >= this.modelConfigs.length) {
                // N·∫øu t·∫•t c·∫£ ƒë·ªÅu failed, reset v√† th·ª≠ l·∫°i t·ª´ ƒë·∫ßu
                if (this.failedConfigs.size === this.modelConfigs.length) {
                    this.failedConfigs.clear();
                    this.currentConfigIndex = 0;
                    return 0;
                }
                // N·∫øu c√≤n config ch∆∞a failed, ti·∫øp t·ª•c t√¨m
                if (attempts >= this.modelConfigs.length * 2) {
                    return null; // Kh√¥ng t√¨m th·∫•y config n√†o
                }
            }
        } while (this.failedConfigs.has(this.currentConfigIndex) && attempts < this.modelConfigs.length * 2);

        return this.currentConfigIndex;
    }

    private async tryWithFallback<T>(
        operation: (model: any) => Promise<T>,
        operationName: string
    ): Promise<T> {
        const maxAttempts = this.modelConfigs.length * 2; // Cho ph√©p th·ª≠ nhi·ªÅu h∆°n m·ªôt l·∫ßn
        let lastError: any;
        let consecutiveFailures = 0;

        for (let attempt = 0; attempt < maxAttempts; attempt++) {
            try {
                const model = this.getCurrentModel();
                const result = await operation(model);

                const config = this.modelConfigs[this.currentConfigIndex];
                console.log(`‚úì ${operationName} succeeded with model: ${config.modelName} (API key ${this.currentConfigIndex + 1})`);

                // Reset failed status n·∫øu th√†nh c√¥ng
                this.failedConfigs.delete(this.currentConfigIndex);

                return result;
            } catch (error: any) {
                lastError = error;
                const config = this.modelConfigs[this.currentConfigIndex];

                if (isQuotaOrRateLimitError(error)) {
                    console.warn(`‚ö† ${operationName} failed with model ${config.modelName} (API key ${this.currentConfigIndex + 1}): ${error.message}`);

                    // ƒê√°nh d·∫•u config n√†y ƒë√£ failed
                    this.failedConfigs.add(this.currentConfigIndex);
                    consecutiveFailures++;

                    // N·∫øu t·∫•t c·∫£ configs ƒë·ªÅu failed, ƒë·ª£i l√¢u h∆°n
                    if (this.failedConfigs.size === this.modelConfigs.length) {
                        const waitTime = Math.min(5000 + (consecutiveFailures * 1000), 30000); // T·ªëi ƒëa 30 gi√¢y
                        console.log(`‚è≥ All configs exhausted. Waiting ${waitTime/1000}s before retry...`);
                        await new Promise(resolve => setTimeout(resolve, waitTime));
                        this.failedConfigs.clear(); // Reset ƒë·ªÉ th·ª≠ l·∫°i
                        this.currentConfigIndex = 0;
                        continue;
                    }

                    // T√¨m config ti·∫øp theo ch∆∞a failed
                    const nextIndex = this.getNextAvailableConfigIndex();
                    if (nextIndex === null) {
                        throw new Error('Kh√¥ng t√¨m th·∫•y config n√†o kh·∫£ d·ª•ng');
                    }

                    // Delay ng·∫Øn tr∆∞·ªõc khi th·ª≠ config ti·∫øp theo
                    await new Promise(resolve => setTimeout(resolve, 500));
                } else {
                    // N·∫øu kh√¥ng ph·∫£i l·ªói quota, throw ngay
                    throw error;
                }
            }
        }

        // N·∫øu ƒë√£ th·ª≠ h·∫øt t·∫•t c·∫£
        throw new Error(
            `T·∫•t c·∫£ c√°c model ƒë√£ h·∫øt quota ho·∫∑c g·∫∑p l·ªói. Vui l√≤ng th·ª≠ l·∫°i sau v√†i ph√∫t. L·ªói cu·ªëi c√πng: ${lastError?.message || 'Unknown error'}`
        );
    }

    async chat(message: string, context?: string, files?: Array<{ mimeType: string; data: Buffer | Uint8Array; fileName?: string }>): Promise<string> {
        const systemInstruction = 'B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh v√† h·ªØu √≠ch. B·∫°n tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch t·ª± nhi√™n v√† chuy√™n nghi·ªáp. Khi ƒë∆∞·ª£c cung c·∫•p context (th√¥ng tin t·ª´ c√°c note), h√£y s·ª≠ d·ª•ng th√¥ng tin ƒë√≥ ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ch√≠nh x√°c v√† chi ti·∫øt. Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch t·ª± nhi√™n v√† chuy√™n nghi·ªáp.';

        let prompt = message;
        if (context) {
            prompt = `D∆∞·ªõi ƒë√¢y l√† c√°c th√¥ng tin context t·ª´ c√°c note m√† ng∆∞·ªùi d√πng ƒë√£ ch·ªçn:\n\n${context}\n\n---\n\nD·ª±a tr√™n context tr√™n, h√£y tr·∫£ l·ªùi c√¢u h·ªèi sau c·ªßa ng∆∞·ªùi d√πng:\n\n${message}`;
        }

        return this.tryWithFallback(async (model) => {
            // Chu·∫©n b·ªã parts cho request
            const parts: any[] = [{ text: prompt }];

            // Th√™m file n·∫øu c√≥
            if (files && files.length > 0) {
                for (const file of files) {
                    parts.push({
                        inlineData: {
                            data: Buffer.from(file.data).toString('base64'),
                            mimeType: file.mimeType
                        }
                    });
                }
            }

            const result = await model.generateContent({
                contents: [{ role: 'user', parts }],
                systemInstruction: systemInstruction,
                generationConfig: {
                    temperature: 0.7,
                    maxOutputTokens: 2000,
                },
            });

            const response = result.response;
            return response.text() || 'Xin l·ªói, t√¥i kh√¥ng th·ªÉ t·∫°o ph·∫£n h·ªìi.';
        }, 'chat');
    }

    async summarize(text: string, maxLength: number = 200): Promise<string> {
        const systemInstruction = 'B·∫°n l√† m·ªôt chuy√™n gia t√≥m t·∫Øt vƒÉn b·∫£n. H√£y t√≥m t·∫Øt vƒÉn b·∫£n m·ªôt c√°ch ng·∫Øn g·ªçn, s√∫c t√≠ch v√† gi·ªØ l·∫°i nh·ªØng th√¥ng tin quan tr·ªçng nh·∫•t.';
        const prompt = `H√£y t√≥m t·∫Øt vƒÉn b·∫£n sau ƒë√¢y trong kho·∫£ng ${maxLength} t·ª´:\n\n${text}`;

        return this.tryWithFallback(async (model) => {
            const result = await model.generateContent({
                contents: [{ role: 'user', parts: [{ text: prompt }] }],
                systemInstruction: systemInstruction,
                generationConfig: {
                    temperature: 0.3,
                    maxOutputTokens: Math.min(maxLength * 2, 1000),
                },
            });

            const response = result.response;
            return response.text() || 'Kh√¥ng th·ªÉ t√≥m t·∫Øt vƒÉn b·∫£n.';
        }, 'summarize');
    }

    async createNote(text: string): Promise<string> {
        const systemInstruction = 'B·∫°n l√† m·ªôt tr·ª£ l√Ω t·∫°o ghi ch√∫ chuy√™n nghi·ªáp. H√£y t·∫°o ghi ch√∫ c√≥ c·∫•u tr√∫c, d·ªÖ ƒë·ªçc v√† d·ªÖ hi·ªÉu t·ª´ vƒÉn b·∫£n ƒë∆∞·ª£c cung c·∫•p. S·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng markdown v·ªõi c√°c ti√™u ƒë·ªÅ, danh s√°ch v√† ƒëi·ªÉm nh·∫•n quan tr·ªçng.';
        const prompt = `H√£y t·∫°o ghi ch√∫ t·ª´ vƒÉn b·∫£n sau:\n\n${text}`;

        return this.tryWithFallback(async (model) => {
            const result = await model.generateContent({
                contents: [{ role: 'user', parts: [{ text: prompt }] }],
                systemInstruction: systemInstruction,
                generationConfig: {
                    temperature: 0.5,
                    maxOutputTokens: 2000,
                },
            });

            const response = result.response;
            return response.text() || 'Kh√¥ng th·ªÉ t·∫°o ghi ch√∫.';
        }, 'createNote');
    }

    async explain(text: string): Promise<string> {
        const systemInstruction = 'B·∫°n l√† m·ªôt gi√°o vi√™n t·∫≠n t√¢m. H√£y gi·∫£i th√≠ch vƒÉn b·∫£n m·ªôt c√°ch d·ªÖ hi·ªÉu, chi ti·∫øt v√† c√≥ v√≠ d·ª• minh h·ªça n·∫øu c·∫ßn.';
        const prompt = `H√£y gi·∫£i th√≠ch chi ti·∫øt vƒÉn b·∫£n sau:\n\n${text}`;

        return this.tryWithFallback(async (model) => {
            const result = await model.generateContent({
                contents: [{ role: 'user', parts: [{ text: prompt }] }],
                systemInstruction: systemInstruction,
                generationConfig: {
                    temperature: 0.6,
                    maxOutputTokens: 2000,
                },
            });

            const response = result.response;
            return response.text() || 'Kh√¥ng th·ªÉ gi·∫£i th√≠ch vƒÉn b·∫£n.';
        }, 'explain');
    }

    async improveWriting(text: string, style: 'formal' | 'casual' | 'academic' | 'professional' = 'professional'): Promise<string> {
        const styleDescriptions = {
            formal: 'trang tr·ªçng, l·ªãch s·ª±',
            casual: 'th√¢n thi·ªán, t·ª± nhi√™n',
            academic: 'h·ªçc thu·∫≠t, chuy√™n s√¢u',
            professional: 'chuy√™n nghi·ªáp, r√µ r√†ng'
        };

        const systemInstruction = `B·∫°n l√† m·ªôt chuy√™n gia bi√™n t·∫≠p vƒÉn b·∫£n. H√£y c·∫£i thi·ªán vƒÉn phong c·ªßa vƒÉn b·∫£n theo phong c√°ch ${styleDescriptions[style]}, gi·ªØ nguy√™n √Ω nghƒ©a nh∆∞ng l√†m cho vƒÉn b·∫£n tr·ªü n√™n hay h∆°n, r√µ r√†ng h∆°n v√† chuy√™n nghi·ªáp h∆°n.`;
        const prompt = `H√£y c·∫£i thi·ªán vƒÉn phong c·ªßa vƒÉn b·∫£n sau theo phong c√°ch ${styleDescriptions[style]}:\n\n${text}`;

        return this.tryWithFallback(async (model) => {
            const result = await model.generateContent({
                contents: [{ role: 'user', parts: [{ text: prompt }] }],
                systemInstruction: systemInstruction,
                generationConfig: {
                    temperature: 0.7,
                    maxOutputTokens: 2000,
                },
            });

            const response = result.response;
            return response.text() || 'Kh√¥ng th·ªÉ c·∫£i thi·ªán vƒÉn b·∫£n.';
        }, 'improveWriting');
    }

    async translate(text: string, targetLanguage: string = 'ti·∫øng Anh'): Promise<string> {
        const systemInstruction = 'B·∫°n l√† m·ªôt chuy√™n gia d·ªãch thu·∫≠t chuy√™n nghi·ªáp. H√£y d·ªãch vƒÉn b·∫£n m·ªôt c√°ch ch√≠nh x√°c, t·ª± nhi√™n v√† gi·ªØ nguy√™n √Ω nghƒ©a.';
        const prompt = `H√£y d·ªãch vƒÉn b·∫£n sau sang ${targetLanguage}:\n\n${text}`;

        return this.tryWithFallback(async (model) => {
            const result = await model.generateContent({
                contents: [{ role: 'user', parts: [{ text: prompt }] }],
                systemInstruction: systemInstruction,
                generationConfig: {
                    temperature: 0.3,
                    maxOutputTokens: 2000,
                },
            });

            const response = result.response;
            return response.text() || 'Kh√¥ng th·ªÉ d·ªãch vƒÉn b·∫£n.';
        }, 'translate');
    }
}

export const geminiService = new GeminiService();

